general:
  seed: 0
  device: 2
  logs_tensorboard: ./results/test/
  models_path: ./results/models_trained/
  cipher: speck # speck, simon, aes228, aes224, gimli, simeck
  nombre_round_eval: 6
  inputs_type: [ctdata0l^ctdata1l, ctdata0r^ctdata1r^ctdata0l^ctdata1l, V0&inv(V1), inv(V0)&inv(V1)]
  #[ctdata0l^ctdata1l, inv(DeltaV), inv(V0)&inv(V1), inv(V0)&V1, inv(DeltaL)] #
  word_size: 16
  alpha: 7
  beta: 2
  type_create_data: normal # real_difference


train_nn:
  retain_model_gohr_ref: No    # Retrain le model de Gohr or load
  load_special: No
  load_nn_path: results/models_trained/speck/6/ctdata0l^ctdata1l_ctdata0r^ctdata1r^ctdata0l^ctdata1l_V0&inv(V1)_inv(V0)&inv(V1)/Gohr_baseline_best_nbre_sampletrain_10000000.pth
  countinuous_learning: No
  curriculum_learning: No
  nbre_epoch_per_stage: 3
  type_model: baseline        # baseline, cnn_attention, multihead, deepset
  nbre_sample_train: 10000000
  nbre_sample_eval: 1000000
  num_epochs: 50
  batch_size: 5000
  loss_type: MSE #BCE - MSE -  SmoothL1Loss CrossEntropyLoss  F1
  lr_nn: 0.001
  weight_decay_nn: 0.00001
  momentum_nn: 0.9 #only for SGD
  optimizer_type: Adam #Adam - AdamW - SGD
  scheduler_type: CyclicLR #CyclicLR - None
  base_lr:  0.0001      #Only if CyclicLR
  max_lr: 0.002        #Only if CyclicLR
  demicycle_1: 6  #Only if CyclicLR
  numLayers: 10
  out_channel0: 32
  out_channel1: 32
  hidden1: 64
  kernel_size0: 1
  kernel_size1: 3
  num_workers: 2
  clip_grad_norm: 0.6
  end_after_training: No


getting_masks:
  load_masks: Yes
  file_mask: results/masks_ref/speck/6/masks_f_6_V0V1_inv_V0_invV1_inv2.txt
  nbre_max_masks_load: 40
  research_new_masks: No
  nbre_generate_data_train_val: 1000
  nbre_necessaire_val_SV: 100
  nbre_max_batch: 100
  liste_segmentation_prediction: ["1, 0.5"]
  liste_methode_extraction: [IntegratedGradients, IntegratedGradients_tunnel, DeepLift, GradientShap, FeatureAblation, Saliency, ShapleyValueSampling, Occlusion]
  #[IntegratedGradients, IntegratedGradients_tunnel, DeepLift, GradientShap, FeatureAblation, Saliency, ShapleyValueSampling, Occlusion]
  #temps get_mask IntegratedGradients 0.6 for 100 samples
  #temps get_mask IntegratedGradients_tunnel 3.7 for 100 samples
  #temps get_mask DeepLift 0.02 for 100 samples
  #temps get_mask GradientShap 0.04 for 100 samples
  #temps get_mask FeatureAblation 0.2 for 100 samples
  #temps get_mask Saliency 0.03 for 100 samples
  #temps get_mask ShapleyValueSampling 4 for 100 samples
  #temps get_mask Occlusion 0.15 for 100 samples
  liste_methode_selection: [PCA, sum, abs_sum, median, abs_median, mean, abs_mean]
  #[PCA, sum, abs_sum, median, abs_median, mean, abs_mean]
  hamming_weigth: [16, 18]
  thr_value: []
  save_fig_plot_feature_before_mask: Yes
  end_after_step2: No


make_ToT:
  create_new_data_for_ToT: Yes
  create_ToT_with_only_sample_from_cipher: Yes
  nbre_sample_create_ToT: 20000000

make_data_classifier:
  create_new_data_for_classifier: Yes
  nbre_sample_train_classifier: 100000
  nbre_sample_val_classifier: 100000

compare_classifer:
  eval_nn_ref: Yes
  retrain_nn_ref: Yes
  save_data_proba: Yes
  num_epch_2: 20
  batch_size_2: 500
  classifiers_ours: [NN, LGBM] # [NN, LGBM, RF]
  num_epch_our: 20
  batch_size_our: 500
  retrain_with_import_features: No
  keep_number_most_impactfull: 30
  quality_of_masks: Yes
  compute_independance_feature: Yes
  alpha_test: 0.05
  end_after_step4: No


prunning:
  model_to_prune: results/models_trained/speck/6/ctdata0l^ctdata1l_ctdata0r^ctdata1r^ctdata0l^ctdata1l_V0&inv(V1)_inv(V0)&inv(V1)/Gohr_baseline_best_nbre_sampletrain_10000000.pth
  values_prunning: [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.55, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95]
  layers_NOT_to_prune: []
  save_model_prune: No
  logs_layers: No
  nbre_sample_eval_prunning: 100000




